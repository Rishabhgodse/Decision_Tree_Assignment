{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6vMvTRfeQnY"
      },
      "outputs": [],
      "source": [
        "# What is a Decision Tree, and how does it work?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It splits the data into smaller subsets based on feature values, resulting in a tree-like structure where each internal node represents a decision based on a feature, branches represent the outcome of the decision, and leaves represent the final prediction."
      ],
      "metadata": {
        "id": "F1ogu7noexdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are impurity measures in Decision Trees?"
      ],
      "metadata": {
        "id": "I-oOuovweyMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impurity measures help decide how to split the data at each node. A pure node contains only one class, while an impure node contains a mix of classes. The goal is to minimize impurity when splitting."
      ],
      "metadata": {
        "id": "3sJM-ewFe4h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the mathematical formula for Gini Impurity?"
      ],
      "metadata": {
        "id": "DK2K_2oMe7tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gini Impurity measures how often a randomly chosen element would be incorrectly classified. Its formula is:\n",
        "\n",
        "Gini\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "Gini=1‚àí\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " p\n",
        "i\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "Where\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "‚Äã\n",
        "  is the probability of class\n",
        "ùëñ\n",
        "i."
      ],
      "metadata": {
        "id": "HUGBprHnfFFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the mathematical formula for Entropy?"
      ],
      "metadata": {
        "id": "2qd_22awfH_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy measures the disorder or randomness in the data. Its formula is:\n",
        "\n",
        "Entropy\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "(\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "Entropy=‚àí\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " p\n",
        "i\n",
        "‚Äã\n",
        " log\n",
        "2\n",
        "‚Äã\n",
        " (p\n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "Where\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "‚Äã\n",
        "  is the probability of class\n",
        "ùëñ\n",
        "i."
      ],
      "metadata": {
        "id": "3CAMudiefMVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "4MueNa5nfPFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain measures how much a feature helps reduce uncertainty (impurity). It‚Äôs the difference between the impurity of the parent node and the weighted impurity of the child nodes. Decision Trees use Information Gain to decide which feature to split on.\n",
        "\n",
        "Information¬†Gain\n",
        "=\n",
        "Entropy¬†(Parent)\n",
        "‚àí\n",
        "Weighted¬†Entropy¬†(Children)\n",
        "Information¬†Gain=Entropy¬†(Parent)‚àíWeighted¬†Entropy¬†(Children)"
      ],
      "metadata": {
        "id": "vLdOKt9NfVpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "fUG_sfvOfYRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini Impurity: Easier to compute, tends to favor larger partitions.\n",
        "\n",
        "Entropy: Based on the concept of information theory, it works similarly but can be more sensitive to skewed distributions. Both are used to measure impurity, but Gini is computationally faster, whereas Entropy can be more accurate for certain datasets."
      ],
      "metadata": {
        "id": "jkTGtno6fcK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mathematical explanation behind Decision Trees?"
      ],
      "metadata": {
        "id": "-vHZ4TpLfkMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree uses recursive binary splitting to minimize the impurity at each node. The splits are chosen to maximize Information Gain or reduce impurity (Gini or Entropy). Mathematically, at each step, the tree evaluates every possible split for all features and chooses the one that minimizes the chosen impurity measure."
      ],
      "metadata": {
        "id": "Ze8wT0YJfp4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "VusPkWdpfsRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Pruning stops the tree from growing once it meets a certain condition (e.g., maximum depth, minimum number of samples at a node). It prevents overfitting by limiting tree size."
      ],
      "metadata": {
        "id": "O4n__hAkf8Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is Post-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "Q8rmS4TwgJiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Post-Pruning allows the tree to grow fully and then removes nodes that don't improve accuracy on a validation set. This also helps prevent overfitting."
      ],
      "metadata": {
        "id": "QkqXeDspgNqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Difference between Pre-Pruning and Post-Pruning?"
      ],
      "metadata": {
        "id": "KbX5sfMNgRZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Pruning: Stops tree growth early, based on set conditions.\n",
        "\n",
        "Post-Pruning: Grows the tree fully first and then removes unnecessary nodes."
      ],
      "metadata": {
        "id": "gbzWnhqKgZm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is a Decision Tree Regressor?"
      ],
      "metadata": {
        "id": "yq7MDDRVgcE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree Regressor predicts continuous values rather than classes. It works similarly to classification trees, but instead of reducing class impurity, it minimizes the variance of the target variable in each split."
      ],
      "metadata": {
        "id": "Q-VFPOJBg2Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Advantages and disadvantages of Decision Trees?"
      ],
      "metadata": {
        "id": "wLbv5JNLg4iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages:\n",
        "\n",
        "Easy to understand and visualize.\n",
        "Handles both categorical and numerical data.\n",
        "No need for feature scaling.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Prone to overfitting (complex trees).\n",
        "Sensitive to small changes in data."
      ],
      "metadata": {
        "id": "jElXZ4D9hOUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How does a Decision Tree handle missing values?"
      ],
      "metadata": {
        "id": "qQD2fHwuhU9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees can handle missing values by either ignoring them during split calculations or using a surrogate split (a backup feature with similar splits) to make decisions."
      ],
      "metadata": {
        "id": "-pNtDleDhY1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  How does a Decision Tree handle categorical features?"
      ],
      "metadata": {
        "id": "ADCNlaBWhbcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For categorical features, Decision Trees can split based on each category or group similar categories together based on Information Gain or Gini Impurity."
      ],
      "metadata": {
        "id": "p-Hh-n5ehenJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "zDBm8b7ahhFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Medical diagnosis: To classify diseases.\n",
        "Customer segmentation: To target marketing efforts.\n",
        "Fraud detection: To detect fraudulent activities in financial systems."
      ],
      "metadata": {
        "id": "mBNf1c-AhmgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                #  Practical"
      ],
      "metadata": {
        "id": "QTKjWRb4hozT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier on Iris dataset and print model accuracy?"
      ],
      "metadata": {
        "id": "X_Ib5_1JiUmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "un8cKIIdipwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier using Gini Impurity and print feature importances?"
      ],
      "metadata": {
        "id": "fgcTICuair3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clf_gini = DecisionTreeClassifier(criterion='gini')\n",
        "clf_gini.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "importances = clf_gini.feature_importances_\n",
        "print(\"Feature Importances:\", importances)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P9gDLG1ViwEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier using Entropy and print model accuracy?"
      ],
      "metadata": {
        "id": "LfoJvyczi2Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clf_entropy = DecisionTreeClassifier(criterion='entropy')\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_entropy = clf_entropy.predict(X_test)\n",
        "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
        "print(f\"Model Accuracy with Entropy: {accuracy_entropy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "TU3RxQSni59s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Regressor on Housing Dataset (MSE Evaluation)?"
      ],
      "metadata": {
        "id": "eG7rAwz4i8ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load housing data\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_reg = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred_reg)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "id": "ettebTSSjAc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier and visualize the tree using graphviz?"
      ],
      "metadata": {
        "id": "TamJyc6YjCnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Train the classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the tree\n",
        "dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names, filled=True, rounded=True, special_characters=True)\n",
        "\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.view()\n"
      ],
      "metadata": {
        "id": "XVSMs2KojI7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier with max depth of 3 and compare with fully grown tree?"
      ],
      "metadata": {
        "id": "9Tw1BGNxjKjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Max depth of 3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "\n",
        "# Fully grown tree\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate both\n",
        "accuracy_depth3 = accuracy_score(y_test, clf_depth3.predict(X_test))\n",
        "accuracy_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "print(f\"Accuracy with max depth 3: {accuracy_depth3 * 100:.2f}%\")\n",
        "print(f\"Accuracy with full tree: {accuracy_full * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "EbmxuzCjjN6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier using min_samples_split=5 and compare with default tree?"
      ],
      "metadata": {
        "id": "85zVzG1IjPpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clf_min_samples = DecisionTreeClassifier(min_samples_split=5)\n",
        "clf_min_samples.fit(X_train, y_train)\n",
        "\n",
        "# Default tree\n",
        "clf_default = DecisionTreeClassifier()\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate both\n",
        "accuracy_min_samples = accuracy_score(y_test, clf_min_samples.predict(X_test))\n",
        "accuracy_default = accuracy_score(y_test, clf_default.predict(X_test))\n",
        "\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_min_samples * 100:.2f}%\")\n",
        "print(f\"Accuracy with default: {accuracy_default * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "LIYlu98ljTfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply feature scaling before training a Decision Tree Classifier and compare accuracy?"
      ],
      "metadata": {
        "id": "IouqHJeBjVST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train with scaled data\n",
        "clf_scaled = DecisionTreeClassifier()\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Compare with unscaled data\n",
        "accuracy_scaled = accuracy_score(y_test, clf_scaled.predict(X_test_scaled))\n",
        "accuracy_unscaled = accuracy_score(y_test, clf.predict(X_test))\n",
        "\n",
        "print(f\"Accuracy with scaled data: {accuracy_scaled * 100:.2f}%\")\n",
        "print(f\"Accuracy with unscaled data: {accuracy_unscaled * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "eZNK85MSjZ1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification?"
      ],
      "metadata": {
        "id": "tVEM5IYWjcca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# OvR strategy\n",
        "clf_ovr = OneVsRestClassifier(DecisionTreeClassifier())\n",
        "clf_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_ovr = clf_ovr.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Model Accuracy with OvR: {accuracy_ovr * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "_gBtESLMj-Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Train Decision Tree Classifier and display feature importance scores?"
      ],
      "metadata": {
        "id": "UiUQ_oo0kANz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "importances = clf.feature_importances_\n",
        "print(\"Feature Importances:\", importances)\n"
      ],
      "metadata": {
        "id": "4L7EvrShkDnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Regressor with max_depth=5 and compare with unrestricted tree?"
      ],
      "metadata": {
        "id": "LI2-SIeFkGAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Max depth of 5\n",
        "regressor_depth5 = DecisionTreeRegressor(max_depth=5)\n",
        "regressor_depth5.fit(X_train, y_train)\n",
        "\n",
        "# Unrestricted regressor\n",
        "regressor_full = DecisionTreeRegressor()\n",
        "regressor_full.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate both\n",
        "mse_depth5 = mean_squared_error(y_test, regressor_depth5.predict(X_test))\n",
        "mse_full = mean_squared_error(y_test, regressor_full.predict(X_test))\n",
        "\n",
        "print(f\"MSE with max_depth=5: {mse_depth5}\")\n",
        "print(f\"MSE with unrestricted tree: {mse_full}\")\n"
      ],
      "metadata": {
        "id": "-SGetfx9kJL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize effect?"
      ],
      "metadata": {
        "id": "lxA55synkLEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with CCP\n",
        "clf_ccp = DecisionTreeClassifier(ccp_alpha=0.01)\n",
        "clf_ccp.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate and visualize CCP effect\n",
        "accuracy_ccp = accuracy_score(y_test, clf_ccp.predict(X_test))\n",
        "print(f\"Accuracy with CCP: {accuracy_ccp * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "JlUocybtkWCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier and evaluate using Precision, Recall, and F1-Score?"
      ],
      "metadata": {
        "id": "cVTr2rEtkXwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "kUIwAb2LkbUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Decision Tree Classifier and visualize confusion matrix using seaborn?"
      ],
      "metadata": {
        "id": "aO7nngnFkdTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Train classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and plot confusion matrix\n",
        "y_pred = clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "72fakwqokg3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GridSearchCV to find optimal values for max_depth and min_samples_split?"
      ],
      "metadata": {
        "id": "lwPrfo96kiX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
        "\n",
        "# Use GridSearchCV\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best accuracy: {grid_search.best_score_}\")\n"
      ],
      "metadata": {
        "id": "mViGhIBwkmkx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nCxjkM-Jko45"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}